{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Esther-Wagatwe/EASY-ML/blob/master/CNN_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import numpy as np # Import the numpy library for numerical operations\n",
        "\n",
        "import torch # Import the PyTorch library\n",
        "from torch import nn # Import the neural network module from PyTorch\n",
        "\n",
        "from torchvision import datasets # Import the datasets module from torchvision, which contains standard datasets like MNIST\n",
        "from torchvision.transforms import ToTensor, transforms # Import the ToTensor transform to convert PIL Images to PyTorch Tensors\n",
        "from torch.utils.data import Subset # Import the Subset class from torch.utils.data\n",
        "from torch.utils.data import DataLoader # Import the DataLoader class from torch.utils.data\n",
        "from sklearn.metrics import accuracy_score # Import the accuracy_score function from scikit-learn\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt # Import the matplotlib.pyplot module for creating plots and visualizations"
      ],
      "metadata": {
        "id": "wbAjz9j_kWmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Load data"
      ],
      "metadata": {
        "id": "F_VEsA2L5DHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk8PaRCYjhxB",
        "outputId": "c3ccd834-3f58-4037-e8ce-d2009be66dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 33.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.04MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 8.79MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.78MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0 - zero',\n",
              " '1 - one',\n",
              " '2 - two',\n",
              " '3 - three',\n",
              " '4 - four',\n",
              " '5 - five',\n",
              " '6 - six',\n",
              " '7 - seven',\n",
              " '8 - eight',\n",
              " '9 - nine']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Setup training data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor() # Define a transform to convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\", # Specify the directory where the dataset will be downloaded\n",
        "    train=True, # Indicate that this is the training dataset\n",
        "    download=True, # Download the dataset if it's not already present\n",
        "    transform=transform, # Apply the ToTensor transform to the loaded images\n",
        "    target_transform=None # No transformation applied to the labels\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\", # Specify the directory where the dataset is located\n",
        "    train=False, # Indicate that this is the test dataset\n",
        "    download=True, # Download the dataset if it's not already present\n",
        "    transform=transform # Apply the ToTensor transform to the loaded images\n",
        ")\n",
        "\n",
        "# See classes\n",
        "class_names = train_data.classes # Get the list of class names (digits 0-9) from the training data\n",
        "class_names # Display the list of class names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the first 10 unique digits from 0 to 9\n",
        "fig, axs = plt.subplots(1, 10, figsize=(16, 2))\n",
        "fig.suptitle(\"First Occurrence of Each Digit (0–9)\", fontsize=14)\n",
        "\n",
        "seen_digits = set()\n",
        "shown = 0\n",
        "\n",
        "# Loop through the dataset and plot the first instance of each class\n",
        "for idx, (img, lbl) in enumerate(train_data):\n",
        "    if lbl not in seen_digits:\n",
        "        axs[shown].imshow(img.squeeze(), cmap=\"gray\")\n",
        "        axs[shown].set_title(f\"Label: {lbl}\")\n",
        "        axs[shown].axis(\"off\")\n",
        "        seen_digits.add(lbl)\n",
        "        shown += 1\n",
        "    if shown == 10:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "iyGEXepy9lyP",
        "outputId": "dd419419-8e48-4a57-f06b-02e170c70bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Data preparation"
      ],
      "metadata": {
        "id": "ruM9EvOJ5HVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 1024 # Define the number of samples per batch\n",
        "num_batches = 2\n",
        "subset_size = BATCH_SIZE * num_batches # Keep 4 batches only\n",
        "\n",
        "all_indices = torch.randperm(len(train_data))  # mixed indices\n",
        "subset_indices = all_indices[:subset_size]     # keep only subset_size\n",
        "\n",
        "# Create Subset\n",
        "train_subset = Subset(train_data, subset_indices)\n",
        "test_subset = Subset(train_data, subset_indices)\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_subset, # Use the training dataset\n",
        "    batch_size=BATCH_SIZE, # Set the batch size for the training data\n",
        "    shuffle=True # Shuffle the data at the beginning of each epoch for better training\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_subset, # Use the test dataset\n",
        "    batch_size=BATCH_SIZE, # Set the batch size for the test data\n",
        "    shuffle=False # No need to shuffle test data for evaluation\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") # Print the DataLoader objects\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") # Print the number of batches in the training DataLoader\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") # Print the number of batches in the test DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny8Z_XDhnF0y",
        "outputId": "fdcc7e6d-12e6-4660-ccbc-b0062421a479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7c8500550cd0>, <torch.utils.data.dataloader.DataLoader object at 0x7c84ffd04e10>)\n",
            "Length of train dataloader: 2 batches of 1024\n",
            "Length of test dataloader: 2 batches of 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Model difinition with and without layers for prevent overfitting"
      ],
      "metadata": {
        "id": "B6Rph0dp5PsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple Convolutional Neural Network (CNN) by subclassing nn.Module\n",
        "class SimpleCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=1, hidden_unit=32, dim_image=28, num_classes=10):\n",
        "        '''\n",
        "        Initializes the model with default or user-defined parameters:\n",
        "         - input_size: number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
        "         - hidden_unit: base number of filters for the convolutional layers\n",
        "         - dim_image: height/width of the input image (assumes square input, e.g., 28x28)\n",
        "         - num_classes: number of output classes for classification\n",
        "        '''\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        '''\n",
        "        This line calls the constructor of the parent class (nn.Module).\n",
        "        It's crucial for proper initialization of the PyTorch module system.\n",
        "        '''\n",
        "\n",
        "        # --- Convolutional Block 1 ---\n",
        "        # First convolutional layer: accepts `input_size` channels (e.g., 1 for grayscale)\n",
        "        # and outputs `hidden_unit` channels with 3x3 kernels and padding to preserve spatial size\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_size, out_channels=hidden_unit, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()  # Apply ReLU activation function\n",
        "\n",
        "        # --- Convolutional Block 2 ---\n",
        "        # Second convolutional layer: doubles the number of channels\n",
        "        self.conv2 = nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit*2, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()  # Apply ReLU activation function\n",
        "\n",
        "        # --- MaxPooling Layer ---\n",
        "        # Downsamples the spatial dimensions by a factor of 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # --- Classifier (Fully Connected Layers) ---\n",
        "        # Compute flattened size after pooling: each dimension is halved\n",
        "        linear_input_size = int((hidden_unit*2) * (dim_image/2) * (dim_image/2))\n",
        "        # First fully connected layer: projects to a higher-dimensional space\n",
        "        self.fc1 = nn.Linear(linear_input_size, hidden_unit*4)\n",
        "        self.relu3 = nn.ReLU()  # Apply ReLU activation function\n",
        "        # Final fully connected layer: maps to number of output classes\n",
        "        self.fc2 = nn.Linear(hidden_unit*4, num_classes)\n",
        "\n",
        "        # Softmax layer not needed here because CrossEntropyLoss includes it\n",
        "        # self.softmax = nn.Softmax(1)\n",
        "\n",
        "    # Define the forward pass through the network\n",
        "    def forward(self, x):\n",
        "        # --- Convolutional Block 1 ---\n",
        "        x = self.conv1(x)  # Apply first convolution\n",
        "        x = self.relu1(x)  # Apply activation\n",
        "\n",
        "        # --- Convolutional Block 2 ---\n",
        "        x = self.conv2(x)  # Apply second convolution\n",
        "        x = self.relu2(x)  # Apply activation\n",
        "\n",
        "        # --- Pooling ---\n",
        "        x = self.pool(x)  # Apply max pooling to reduce spatial size\n",
        "\n",
        "        # --- Flattening ---\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch size\n",
        "\n",
        "        # --- Fully Connected Layers ---\n",
        "        x = self.fc1(x)    # First dense layer\n",
        "        x = self.relu3(x)  # Activation\n",
        "        x = self.fc2(x)    # Final output layer\n",
        "\n",
        "        # Softmax not applied explicitly since CrossEntropyLoss expects raw logits\n",
        "        # x = self.softmax(x)\n",
        "\n",
        "        return x  # Return output logits"
      ],
      "metadata": {
        "id": "u85zvfUd5e7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an advanced CNN with batch normalization and dropout for better generalization\n",
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_unit=32, dim_image=28, num_classes=10):\n",
        "        '''\n",
        "          Initializes the model with default or user-defined parameters:\n",
        "          - input_size: number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
        "          - hidden_unit: base number of filters for the convolutional layers\n",
        "          - dim_image: height/width of the input image (assumes square input, e.g., 28x28)\n",
        "          - num_classes: number of output classes for classification\n",
        "        '''\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        '''\n",
        "        This line calls the constructor of the parent class (nn.Module).\n",
        "        It's crucial for proper initialization of the PyTorch module system.\n",
        "        '''\n",
        "\n",
        "        # --- Convolutional Block 1 ---\n",
        "        # First convolutional layer: transforms input channels to `hidden_unit` feature maps\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_size, out_channels=hidden_unit, kernel_size=3, padding=1)\n",
        "        # Batch normalization to stabilize and accelerate training\n",
        "        self.bn1 = nn.BatchNorm2d(hidden_unit)\n",
        "        # ReLU activation to introduce non-linearity\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Dropout to prevent overfitting (25% of features randomly zeroed)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "\n",
        "        # --- Convolutional Block 2 ---\n",
        "        # Second convolutional layer: increases channels to hidden_unit * 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit*2, kernel_size=3, padding=1)\n",
        "        # Batch normalization for the new number of channels\n",
        "        self.bn2 = nn.BatchNorm2d(hidden_unit*2)\n",
        "        # ReLU activation\n",
        "        self.relu2 = nn.ReLU()\n",
        "        # Dropout again to regularize\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        # --- MaxPooling Layer ---\n",
        "        # Downsample the feature maps by a factor of 2 (halves width and height)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # --- Fully Connected Block ---\n",
        "        # Calculate the input size for the first dense layer after flattening\n",
        "        linear_input_size = int((hidden_unit*2) * (dim_image/2) * (dim_image/2))\n",
        "        # First dense (fully connected) layer to project features to a higher dimension\n",
        "        self.fc1 = nn.Linear(linear_input_size, hidden_unit*4)\n",
        "        # Batch normalization on fully connected output\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_unit*4)\n",
        "        # ReLU activation\n",
        "        self.relu3 = nn.ReLU()\n",
        "        # Dropout to regularize the dense layer (50% dropout rate)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        # Final dense layer to map to the number of classes\n",
        "        self.fc2 = nn.Linear(hidden_unit*4, num_classes)\n",
        "        # Softmax to convert raw logits to class probabilities (note: usually done outside model with CrossEntropyLoss)\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    # Define the forward pass through the model\n",
        "    def forward(self, x):\n",
        "        # --- Convolutional Block 1 ---\n",
        "        x = self.conv1(x)      # Apply first convolution\n",
        "        x = self.bn1(x)        # Apply batch normalization\n",
        "        x = self.relu1(x)      # Apply ReLU activation\n",
        "        x = self.dropout1(x)   # Apply dropout\n",
        "\n",
        "        # --- Convolutional Block 2 ---\n",
        "        x = self.conv2(x)      # Apply second convolution\n",
        "        x = self.bn2(x)        # Apply batch normalization\n",
        "        x = self.relu2(x)      # Apply ReLU activation\n",
        "        x = self.dropout2(x)   # Apply dropout\n",
        "\n",
        "        # --- Pooling ---\n",
        "        x = self.pool(x)       # Downsample feature maps\n",
        "\n",
        "        # --- Flattening ---\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch size\n",
        "\n",
        "        # --- Fully Connected Block ---\n",
        "        x = self.fc1(x)        # First dense layer\n",
        "        x = self.bn3(x)        # Batch normalization\n",
        "        x = self.relu3(x)      # ReLU activation\n",
        "        x = self.dropout3(x)   # Dropout\n",
        "        x = self.fc2(x)        # Final dense layer\n",
        "        x = self.softmax(x)    # Convert to class probabilities\n",
        "\n",
        "        return x  # Return the final output"
      ],
      "metadata": {
        "id": "FndC2OxWkrWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for GPU availability and select GPU if available"
      ],
      "metadata": {
        "id": "CQeoC4ho_zLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dozr-IuZp-UT",
        "outputId": "004fab01-5494-4e4f-a34d-e0be5548b40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate models and move them to the device (CPU or GPU)"
      ],
      "metadata": {
        "id": "O3EnfSOv_6vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate models :\n",
        "# Wihout Batch Normalization and dropout\n",
        "model_simple = SimpleCNN(input_size=1, hidden_unit=32, dim_image=28, num_classes=10) # Create an instance of the SimpleCNN model defined earlier\n",
        "model_simple = SimpleCNN().to(device) # Move all variables of SimpleCNN to device\n",
        "\n",
        "# With Batch Normalization and dropout\n",
        "model_advanced = AdvancedCNN(input_size=1, hidden_unit=32, dim_image=28, num_classes=10) # Create an instance of the AdvancedCNN model defined earlier\n",
        "model_advanced = AdvancedCNN().to(device) # Move all variables of SimpleCNN to device\n"
      ],
      "metadata": {
        "id": "TIVYk8jRm3K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Train and evaluate the model. We want to train our two models separately, so to write the training procedure only once, we can integrate it into a function that we'll call later.\n"
      ],
      "metadata": {
        "id": "Pmi9Ivy5_hwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_procedure(model, train_dataloader, test_dataloader, epochs, optimizer, loss_fn, device):\n",
        "    # Storage for plotting\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # Training and testing loop\n",
        "    for epoch in range(epochs): # Loop through each epoch\n",
        "        for X_train, y_train in train_dataloader : # Iterate through batches of training data\n",
        "            running_loss = 0.0\n",
        "            train_preds = []\n",
        "            train_labels = []\n",
        "\n",
        "            X_train = X_train.to(device)\n",
        "            y_train = y_train.to(device)\n",
        "            model.train() # Set the model to training mode (enables dropout, batch normalization, etc.)\n",
        "            outputs = model(X_train) # Perform a forward pass to get model outputs (predictions)\n",
        "            loss = loss_fn(outputs, y_train) # Calculate the loss between the model outputs and the true labels\n",
        "\n",
        "            optimizer.zero_grad() # Zero the gradients of the optimizer\n",
        "            loss.backward() # Perform backpropagation to calculate gradients\n",
        "            optimizer.step() # Update the model's parameters based on the calculated gradients\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            train_preds.append(preds.cpu())\n",
        "            train_labels.append(y_train.cpu())\n",
        "\n",
        "        # Compute average training loss and accuracy\n",
        "        train_loss_epoch = running_loss / len(train_dataloader)\n",
        "        train_losses.append(train_loss_epoch)\n",
        "\n",
        "        train_preds = torch.cat(train_preds)\n",
        "        train_labels = torch.cat(train_labels)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Evaluation loop\n",
        "        model.eval()\n",
        "        test_preds = []\n",
        "        test_labels = []\n",
        "        test_loss_total = 0.0\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for X_test, y_test in test_dataloader:\n",
        "                X_test = X_test.to(device)\n",
        "                y_test = y_test.to(device)\n",
        "\n",
        "                test_outputs = model(X_test)\n",
        "                test_loss = loss_fn(test_outputs, y_test)\n",
        "                test_loss_total += test_loss.item()\n",
        "\n",
        "                preds = torch.argmax(test_outputs, dim=1)\n",
        "                test_preds.append(preds.cpu())\n",
        "                test_labels.append(y_test.cpu())\n",
        "\n",
        "        test_loss_epoch = test_loss_total / len(test_dataloader)\n",
        "        test_losses.append(test_loss_epoch)\n",
        "\n",
        "        test_preds = torch.cat(test_preds)\n",
        "        test_labels = torch.cat(test_labels)\n",
        "        test_acc = accuracy_score(test_labels, test_preds)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss_epoch:.4f} | Train Acc: {train_acc:.4f} | Test Loss: {test_loss_epoch:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
        "\n"
      ],
      "metadata": {
        "id": "zmaxiF1b_e2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of loss function and optimizer"
      ],
      "metadata": {
        "id": "qzafsBzQCMqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss() # Define the loss function as Cross-Entropy Loss, commonly used for classification\n",
        "optimizer_model_simple = torch.optim.Adam(model_simple.parameters(), lr=0.01) # Define the optimizer as Adam, with a learning rate of 0.01. It will update the model's parameters.\n",
        "optimizer_model_advanced = torch.optim.Adam(model_advanced.parameters(), lr=0.01) # Define the optimizer as Adam, with a learning rate of 0.01. It will update the model's parameters.\n",
        "epochs = 10 # Define the number of training epochs"
      ],
      "metadata": {
        "id": "ksUDuHh0CEFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train simple model (without Batch Normalization and Dropout)"
      ],
      "metadata": {
        "id": "qGFmIFL1D-H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_simple, test_losses_simple, train_accuracies_simple, test_accuracies_simple = train_and_test_procedure(model_simple,\n",
        "                                                                                                                    train_dataloader,\n",
        "                                                                                                                    test_dataloader,\n",
        "                                                                                                                    epochs,\n",
        "                                                                                                                    optimizer_model_simple,\n",
        "                                                                                                                    loss_fn,\n",
        "                                                                                                                    device)"
      ],
      "metadata": {
        "id": "idx3380sC6Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train advanced model (with Batch Normalization and Dropout)"
      ],
      "metadata": {
        "id": "lprDSi_zEHME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_advanced, test_losses_advanced, train_accuracies_advanced, test_accuracies_advanced = train_and_test_procedure(model_advanced,\n",
        "                                                                                                                            train_dataloader,\n",
        "                                                                                                                            test_dataloader,\n",
        "                                                                                                                            epochs,\n",
        "                                                                                                                            optimizer_model_advanced,\n",
        "                                                                                                                            loss_fn,\n",
        "                                                                                                                            device)"
      ],
      "metadata": {
        "id": "vxrgD__FEKwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Visualisation of results\n",
        "For the same purpose we create a function for displaying more easily the evolution of the loss and the accuracy for the different models.\n",
        "\n"
      ],
      "metadata": {
        "id": "5gBlzB10EP8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_and_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # --- Loss Plot ---\n",
        "    # Left Y-axis: Train Loss\n",
        "    ax1.plot(train_losses, label='Train Loss', color='tab:orange')\n",
        "    ax1.set_ylabel('Train Loss', color='tab:orange')\n",
        "    ax1.tick_params(axis='y', labelcolor='tab:orange')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_title('Loss over Epochs')\n",
        "\n",
        "    # Right Y-axis: Test Loss\n",
        "    ax1b = ax1.twinx()\n",
        "    ax1b.plot(test_losses, label='Test Loss', color='tab:blue')\n",
        "    ax1b.set_ylabel('Test Loss', color='tab:blue')\n",
        "    ax1b.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "    # Optional: combine legends\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax1b.get_legend_handles_labels()\n",
        "    ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "\n",
        "    # --- Accuracy Plot ---\n",
        "    ax2.plot(train_accuracies, label='Train Accuracy', color='tab:orange')\n",
        "    ax2.plot(test_accuracies, label='Test Accuracy', color='tab:blue')\n",
        "    ax2.set_title('Accuracy over Epochs')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    fig.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HwNi79GlEQlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Model without Batch Normalization and dropout\"\n",
        "plot_loss_and_accuracy(train_losses_simple, test_losses_simple, train_accuracies_simple, test_accuracies_simple, title)\n",
        "\n",
        "title = \"Model with Batch Normalization and dropout\"\n",
        "plot_loss_and_accuracy(train_losses_advanced, test_losses_advanced, train_accuracies_advanced, test_accuracies_advanced, title)"
      ],
      "metadata": {
        "id": "1FT45Km4GJec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 1:\n",
        "Analyze the results, what happened during the training and why ?"
      ],
      "metadata": {
        "id": "g8whzQ4cJSUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 2:\n",
        "Create new models by testing :\n",
        "\n",
        "* suppress batch normalization on dense layers\n",
        "* use only batch normalization on convolutional layers\n",
        "* use only dropouts and supress the batch normalization\n",
        "\n",
        "If you have access to GPU, you can also try to increase the size of your dataset\n",
        "\n",
        "Train some of theses new models and compare the evolution of loss and accuracy"
      ],
      "metadata": {
        "id": "bcQlnKF-J7Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your new Convolutional Neural Network (CNN)\n",
        "class NewCNN(nn.Module):\n",
        "    # Initialize your layers\n",
        "    def __init__(self, input_size=1, hidden_unit=32, dim_image=28, num_classes=10):\n",
        "        super(NewCNN, self).__init__()\n",
        "\n",
        "\n",
        "    # Define the forward pass through the network\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        return x  # Return output logits"
      ],
      "metadata": {
        "id": "77vKsihiJ63-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate model\n",
        "# your code here\n",
        "\n",
        "# Define your loss function, optimizer and number of epoch (maintain at 10)\n",
        "# your code here\n",
        "\n",
        "# train and evaluate your model\n",
        "# your code here\n",
        "\n",
        "# visualize your results\n",
        "# your code here"
      ],
      "metadata": {
        "id": "MCtBPAwDL2Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus : data augmentation\n",
        "Try to apply data augmentation to your fruits dataset"
      ],
      "metadata": {
        "id": "Q9MvqH8cL14R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## PLEASE DON'T MODIFY THIS CELL\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
        "\n",
        "# Paths and file ID\n",
        "dataset_folder = \"/content/fruits_dataset\"\n",
        "zip_path = \"/content/fruits_dataset.zip\"\n",
        "file_id = \"153Z20lsYzdpKHlFSrRlBWsSJNaXhcl6a\"\n",
        "\n",
        "# Download only if zip not already present\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    gdown.download(id=file_id, output=zip_path, fuzzy=True)\n",
        "else:\n",
        "    print(\"Zip file already exists. Skipping download.\")\n",
        "\n",
        "# Extract only if fruits_dataset/ does not already exist\n",
        "if not os.path.exists(dataset_folder):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "# Move apples and pears into fruits_dataset/ if they exist directly under /content\n",
        "for folder in [\"apples\", \"pears\", \"Pears\"]:\n",
        "    src = f\"/content/{folder}\"\n",
        "    dst = f\"/content/fruits_dataset/{folder.lower()}\"\n",
        "    if os.path.exists(src) and not os.path.exists(dst):\n",
        "        os.makedirs(\"/content/fruits_dataset\", exist_ok=True)\n",
        "        os.rename(src, dst)\n",
        "\n",
        "# Confirm class folders\n",
        "print(\"Available classes:\", os.listdir(\"/content/fruits_dataset\"))\n"
      ],
      "metadata": {
        "id": "_fobPpijOWGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define original transform (to apply to all images)\n",
        "# --- Base transform (mild preprocessing) ---\n",
        "original_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# --- Define separate augmentation transforms ---\n",
        "transform_rotation = transforms.Compose([\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_translation = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_hflip = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_vflip = transforms.Compose([\n",
        "    transforms.RandomVerticalFlip(p=1.0),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_blur = transforms.Compose([\n",
        "    transforms.GaussianBlur(kernel_size=125, sigma=3),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_color_jitter = transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=0.9, contrast=0.4, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "97YYE-4CMzjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_dir = \"/content/fruits_dataset\"\n",
        "\n",
        "# Load original dataset\n",
        "original_dataset     = datasets.ImageFolder(root=data_dir, transform=original_transform)\n",
        "\n",
        "# Create augmented version of the same dataset\n",
        "rotated_dataset      = datasets.ImageFolder(root=data_dir, transform=transform_rotation)\n",
        "translated_dataset   = datasets.ImageFolder(root=data_dir, transform=transform_translation)\n",
        "hflipped_dataset     = datasets.ImageFolder(root=data_dir, transform=transform_hflip)\n",
        "vflipped_dataset     = datasets.ImageFolder(root=data_dir, transform=transform_vflip)\n",
        "blur_dataset         = datasets.ImageFolder(root=data_dir, transform=transform_blur)\n",
        "color_jitter_dataset = datasets.ImageFolder(root=data_dir, transform=transform_color_jitter)\n",
        "\n",
        "\n",
        "# Combine all datasets to expand the training data\n",
        "combined_dataset = ConcatDataset([\n",
        "    original_dataset,\n",
        "    rotated_dataset,\n",
        "    translated_dataset,\n",
        "    hflipped_dataset,\n",
        "    vflipped_dataset,\n",
        "    blur_dataset,\n",
        "    color_jitter_dataset\n",
        "\n",
        "])\n",
        "\n",
        "# Split into train/validation\n",
        "train_size = int(0.8 * len(combined_dataset))\n",
        "val_size = len(combined_dataset) - train_size\n",
        "train_ds, val_ds = random_split(combined_dataset, [train_size, val_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16)\n",
        "\n",
        "# Class info and sizes\n",
        "class_names = original_dataset.classes\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Original dataset size:     \", len(original_dataset))\n",
        "print(\"Rotation dataset size:     \", len(rotated_dataset))\n",
        "print(\"Translation dataset size:  \", len(translated_dataset))\n",
        "print(\"H-Flip dataset size:       \", len(hflipped_dataset))\n",
        "print(\"V-Flip dataset size:       \", len(vflipped_dataset))\n",
        "print(\"Blur dataset size:         \", len(blur_dataset))\n",
        "print(\"Color Jiter dataset size:  \", len(color_jitter_dataset))\n",
        "print(\"Final combined dataset size:\", len(combined_dataset))\n",
        "print(\"Train set size:            \", len(train_ds))\n",
        "print(\"Validation set size:       \", len(val_ds))"
      ],
      "metadata": {
        "id": "yVbSvkAfOXdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of one original image and augmented versions"
      ],
      "metadata": {
        "id": "su2swD2pSIYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# load one image\n",
        "img_path, label = original_dataset.samples[0]\n",
        "img = Image.open(img_path)\n",
        "\n",
        "# Apply transforms (convert back to PIL for display)\n",
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "images = {\n",
        "    \"Original\": img,\n",
        "    \"Rotation\": to_pil(transform_rotation(img)),\n",
        "    \"Translation\": to_pil(transform_translation(img)),\n",
        "    \"H-Flip\": to_pil(transform_hflip(img)),\n",
        "    \"V-Flip\": to_pil(transform_vflip(img)),\n",
        "    \"Blur\": to_pil(transform_blur(img)),\n",
        "    \"Color Jitter\": to_pil(transform_color_jitter(img))\n",
        "}\n",
        "\n",
        "# Plot all images in a row\n",
        "plt.figure(figsize=(15, 3))\n",
        "for i, (title, image) in enumerate(images.items()):\n",
        "    plt.subplot(1, len(images), i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zldjN6xQR3ok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}